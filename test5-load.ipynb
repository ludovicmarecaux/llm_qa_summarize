{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'philo.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfloader=PyPDFLoader(text)\n",
    "doc=pdfloader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=doc[5:71]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "\n",
    "for page in doc:\n",
    "    text += page.page_content\n",
    "    \n",
    "text = text.replace('\\t', ' ')\n",
    "#text = text.replace('\\u202f', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\multiplellm\\multiplellm\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.1,'max_new_tokens':4000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import CTransformers\n",
    "#llm=CTransformers(model='model/llama-2-7b-chat.ggmlv3.q8_0.bin',\n",
    "                      #model_type='llama',\n",
    "                      #config={'temperature':0,'context_length': 4096}\n",
    "                      #)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (31897 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31897"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                                                separators=['\\n\\n', '\\n', '(?=>\\. )', ' ', ''], \n",
    "                                               chunk_size=1000, \n",
    "                                               chunk_overlap=500\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 170 documents and the first one has 349 tokens\n"
     ]
    }
   ],
   "source": [
    "print (f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os\n",
    "PATH_DB=os.path.join(os.getcwd(),\"save_vectorstore_philo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#embeddings= HuggingFaceEmbeddings(model_name=\"intfloat/e5-large-v2\")\n",
    "embeddings=HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
    "vectorstore= Chroma.from_documents(documents=docs,embedding=embeddings,persist_directory=PATH_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour load un vectorstore\n",
    "#from langchain.vectorstores import Chroma\n",
    "#import os\n",
    "#PATH_DB=os.path.join(os.getcwd(),\"save_vectorstore\")\n",
    "#vectordb222 = Chroma(persist_directory=PATH_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "retireval_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "chain=load_qa_chain(llm,chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query=\" donne les points clés à retenir du document? il faudra faire abstraction des mentions légales et la réponse doit être formulé par un spécialiste du bâtiment et rédigé en français \"\n",
    "#query=\"dresse un bilan de la situation en angleterre décrit dans le document, la réponse doit être ilustré avec au moins 10 pourcentages et formulé par un spécialiste du bâtiment et rédigé en français\"\n",
    "query=\"tu es un grand philosophe et tu expliciteras les axes de réflexions concernant la thématique entre l'histoire et la liberté? réponse en francais  \"\n",
    "matching_results=vectorstore.similarity_search(query,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.run(input_documents=matching_results,question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Certainement, je vais vous présenter les axes de réflexion concernant la thématique entre l'histoire et la liberté.\n",
      "\n",
      "1. L'histoire comme facteur de la libération de l'homme:\n",
      "- Histoire, source d'instruction: L'histoire peut être un outil d'apprentissage et d'éducation, en nous permettant de comprendre les événements passés et de tirer des leçons pour l'avenir.\n",
      "- Histoire, source d'inspiration: L'histoire peut également être une source d'inspiration, en nous montrant des exemples de personnes qui ont lutté pour leur liberté et qui ont réussi à surmonter les obstacles.\n",
      "\n",
      "2. L'histoire comme facteur d'aliénation:\n",
      "- Histoire, source de nostalgie: L'histoire peut être une source de nostalgie, en nous faisant regretter un passé idéalisé et en nous empêchant de nous concentrer sur le présent et l'avenir.\n",
      "- Histoire, source de traumatisme: L'histoire peut également être une source de traumatisme, en nous rappelant des événements douloureux et en nous empêchant de tourner la page.\n",
      "\n",
      "3. Philosophie de l'histoire comme moteur de l'histoire:\n",
      "- Les philosophies de l'histoire peuvent donner du sens à l'existence humaine, en offrant une direction et une signification à l'histoire.\n",
      "\n",
      "En somme, l'histoire peut être à la fois un outil de libération et un facteur d'aliénation, tout dépend de la manière dont on l'aborde et dont on l'utilise. La philosophie de l'histoire peut également jouer un rôle important en offrant une compréhension plus profonde de l'histoire et de son sens.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The hypotheses of transformation of the surfaces of the parc of bureaux franciliens are between 0.6% and 1.3% of the total of the surfaces of the parc of bureaux franciliens.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = retireval_chain({\"query\": query})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_chain = load_summarize_chain(llm=llm, chain_type='map_reduce',\n",
    "                                      #verbose=True # Set verbose=True if you want to see the prompts being used\n",
    "                                    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = summary_chain.run(input_documents=docs,token_max=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=150\n",
    "#[output[i:i+n] for i in range(0, len(output), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "map_prompt =  \"[INST] tu es un philosophe. Ecrit un résumé précis en francais de :{text}Résumé précis: [/INST]\"\n",
    "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_prompt = \"[INST] tu es un philosophe. Ecrit un résumé en francais en 200 mots de :{text}Résumé précis: [/INST]\"\n",
    "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "summary_chain = load_summarize_chain(llm=llm,\n",
    "                                     chain_type='map_reduce',\n",
    "                                     map_prompt=map_prompt_template,\n",
    "                                     combine_prompt=combine_prompt_template,\n",
    "#                                      verbose=True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#summary_chain = load_summarize_chain(llm=llm,\n",
    "                                     #chain_type='refine',\n",
    "                                     #refine_prompt=map_prompt_template,\n",
    "                                     \n",
    "#                                      verbose=True\n",
    "                                    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = summary_chain.run(input_documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La dissertation philosophique et le commentaire philosophique sont deux exercices qui permettent de développer des compétences d'argumentation, d'analyse et de réflexion critique. La dissertation consiste à défendre une thèse sur un sujet donné, en utilisant des arguments logiques et rationnels, tandis que le commentaire vise à analyser et interpréter un texte philosophique.\n",
      "\n",
      "Ces exercices permettent de développer des compétences utiles dans divers aspects de la vie, en particulier la capacité à argumenter de manière claire et convaincante. La philosophie aborde des questions centrales telles que la question de l'universalité de la morale ou l'existence de Dieu, qui soulèvent des enjeux théoriques et pratiques considérables.\n",
      "\n",
      "La lecture attentive et structurée des textes philosophiques est essentielle pour en comprendre la logique argumentative. Des exemples de philosophes et de leurs contributions importantes à des questions philosophiques centrales sont présentés, tels que John Locke sur la légitimité du pouvoir politique, Blaise Pascal sur la difficulté de vivre pleinement dans le présent, et Jean-Jacques Rousseau et Montesquieu sur l'importance de la séparation des pouvoirs.\n",
      "\n",
      "D'autres problématiques sont également abordées, telles que la compatibilité entre l'inconscient freudien et la liberté humaine, la relation entre les progrès techniques et l'amélioration de la vie humaine, et le rôle et le statut de l'égalité entre les Hommes dans la démocratie. Enfin, la relation entre l'histoire, l'instruction, l'inspiration, la libération, l'aliénation, la nostalgie et le traumatisme est également soulevée.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
